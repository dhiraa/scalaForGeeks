{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor:\n",
    "\n",
    "```\n",
    " Spark Cluster -> Worker Node 1 (Slave Machines) -> Runs Executor 1 with n slots/cores \n",
    "                                                       -> Runs application task/slot send from SparkContext\n",
    "                                                 -> Runs Executor 2 with n slots/cores\n",
    "                                                 -> Runs Executor ...\n",
    "               -> Worker Node ...(Slave Machines)\n",
    "\n",
    "                       ----------------------------------------------------\n",
    "                       \\                                                  \\\n",
    "                       \\                                      <----->  Spark Cluster(s)\n",
    " Driver Program (SparkContext)    <----->   Cluster Manager   <----->  Spark Cluster(s)\n",
    "                       \\                                       <-----> Spark Cluster(s)\n",
    "                       \\                                                  \\\n",
    "                       ----------------------------------------------------\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four stages\n",
    "\n",
    "|RDD Objects          |    DAGScheduler         |      TaskScheduler         |    Worker |\n",
    "|---------------------|:-------------------------|----------------------------|------------|\n",
    "|Build operator DAG   | Split graph into stages | Launch tasks via cluster   | Execute tasks|\n",
    "|                     | of tasks                | manager                    |Store and serve blocks\n",
    "|                     |Submit each stage as     | Retry failed or straggling |\n",
    "|                     |ready                    | tasks                      |\n",
    "|                     | Agnostics to operators  | Doesn't know about stages  |\n",
    "\n",
    "\n",
    "1 . The first layer is the interpreter/main program, Spark uses a Scala interpreter, with some modifications.\n",
    "    As  you enter your code in spark console(creating RDD's and applying operators), Spark creates a operator graph.\n",
    "2.  When the user runs an action(like collect), the Graph is submitted to a DAG Scheduler. The DAG scheduler divides\n",
    "    operator graph into (map and reduce) stages.\n",
    "    A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together\n",
    "    to optimize the graph. For e.g. Many map operators can be scheduled in a single stage. This optimization is key to\n",
    "    Sparks performance. The final result of a DAG scheduler is a set of stages.\n",
    "3.  The stages are passed on to the Task Scheduler. The task scheduler launches tasks via cluster manager.\n",
    "    ( Spark Standalone/Yarn/Mesos). The task scheduler doesn't know about dependencies among stages.\n",
    "4.  The Worker executes the tasks on the Slave. A new JVM is started per JOB. The worker knows only about the code that\n",
    "    is passed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference:\n",
    "- Application:- User program built on Spark. Consists of a driver program and executors on the cluster.\n",
    "- Application jar:-  A jar containing the user's Spark application. In some cases users will want to create an \"uber jar\" containing their application along with its dependencies. The user's jar should never include Hadoop or Spark libraries, however, these will be added at runtime.\n",
    "- Driver Program: - The process running the main() function of the application and creating the SparkContext or The program/process responsible for running the Job over the Spark Engine\n",
    "- Cluster manager:-  An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)\n",
    "- Deploy mode:-  Distinguishes where the driver process runs. In \"cluster\" mode, the framework launches the driver inside of the cluster. In \"client\" mode, the submitter launches the driver outside of the cluster.\n",
    "- Worker Node:- Any node that can run application code in the cluster\n",
    "- Executor :- The process responsible for executing a task or A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.\n",
    "- Tasks:- Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor(machine).\n",
    "- Job:-  A piece of code which reads some input  from HDFS or local, performs some computation on the data and writes some output data or A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you'll see this term used in the driver's logs.\n",
    "- Stages:- Jobs are divided into stages. Stages are classified as a Map or reduce stages(Its easier to understand if you have worked on Hadoop and want to correlate). Stages are divided based on computational boundaries, all computations(operators) cannot be Updated in a single Stage. It happens over many stages.\n",
    "- DAG: - DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\n",
    "- Master: - The machine on which the Driver program runs\n",
    "- Slave: - The machine on which the Executor program runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web UI\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val logFile = \"../data/datascience.stackexchange.com/Posts.xml\" \n",
    "val outputPath = \"../data/output/hw_wordcount_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines : 2053\n",
      "Lines with a: 2050, Lines with b: 1982\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "val logData = sc.textFile(logFile, 10).cache() \n",
    "//10 partitions which mean 10 tasks can do this work parallelly, wow!\n",
    "\n",
    "println(\"Number of lines : \" + logData.count()) //Job 0\n",
    "\n",
    "val numAs = logData.filter(line => line.contains(\"a\")).count() //Job 1\n",
    "val numBs = logData.filter(line => line.contains(\"b\")).count() //Job 2\n",
    "println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n",
    "\n",
    "val dir = new File(outputPath)\n",
    "if (dir.exists)\n",
    "dir.delete()\n",
    "\n",
    "sc.textFile(logFile).\n",
    "    flatMap(_.split(\" \")).\n",
    "    map((_, 1)).\n",
    "    reduceByKey(_ + _).\n",
    "    saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToreeSpark - Scala",
   "language": "scala",
   "name": "toreespark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
