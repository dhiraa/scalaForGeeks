{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(20, 19, 18, 17, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nums = List(1 to 20: _*)\n",
    "val rdd = sc.parallelize(nums,5)\n",
    "\n",
    "//SELECT * FROM table ORDER BY col4 DESC LIMIT 10;\n",
    "rdd.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take\n",
    "Extracts the first n items of the RDD and returns them as an array. (Note: This sounds\n",
    "very easy, but it is actually quite a tricky problem for the implementors of Spark because\n",
    "the items in question can be in many different partitions.)\n",
    "\n",
    "### takeOrdered\n",
    "Orders the data items of the RDD using their inherent implicit ordering function and\n",
    "returns the first n items as an array.\n",
    "\n",
    "### takeSample\n",
    "Behaves different from sample in the following respects:\n",
    "• It will return an exact number of samples (Hint: 2nd parameter).\n",
    "• It returns an Array instead of RDD.\n",
    "• It internally randomizes the order of the items returned.\n",
    "\n",
    "### top\n",
    "Utilizes the implicit ordering of T to determine the top k values and returns them as an\n",
    "array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "cat\n",
      "ape\n",
      "cat\n",
      "764,815,274,452,39,538,238,544,475,480,416,868,517,363,39,316,37,90,210,202,335,773,572,243,354,305,584,820,528,749,188,366,913,667,214,540,807,738,204,968,39,863,541,703,397,489,172,29,211,542,600,977,941,923,900,485,575,650,258,31,737,155,685,562,223,675,330,864,291,536,392,108,188,408,475,565,873,504,34,343,79,493,868,974,973,110,587,457,739,745,977,800,783,59,276,987,160,351,515,901\n",
      "9,8\n"
     ]
    }
   ],
   "source": [
    "val b = sc.parallelize(List(\"dog\", \"cat\", \"ape\", \"salmon\", \"gnu\"), 2)\n",
    "b.take(2).foreach(println)\n",
    "\n",
    "val b1 = sc.parallelize(List(\"dog\", \"cat\", \"ape\", \"salmon\", \"gnu\"),2)\n",
    "b1.takeOrdered(2).foreach(println)\n",
    "\n",
    "val x = sc.parallelize(1 to 1000,3)\n",
    "println()\n",
    "println(x.takeSample(true,100,1).mkString(\",\"))\n",
    "\n",
    "val c = sc.parallelize(Array(6, 9, 4, 7, 5, 8) ,2)\n",
    "println()\n",
    "println(c.top(2).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "\n",
    "Evaluates a boolean function for each data item of the RDD and puts the items for\n",
    "which the function returned true into the resulting RDD.\n",
    "\n",
    "When you provide a filter function, it must be able to handle all data items contained\n",
    "in the RDD. Scala provides so-called partial functions to deal with mixed data-types.\n",
    "(Tip: Partial functions are very useful if you have some data which may be bad and\n",
    "you do not want to handle but for the good data (matching data) you want to apply\n",
    "some kind of map function. The following article is good. It teaches you about partial\n",
    "functions in a very nice way and explains why case has to be used for partial functions:\n",
    "http://blog.bruchez.name/2011/10/scala-partial-functions-without-phd.html)\n",
    "\n",
    " filterWith : Deprecated \"use mapPartitionsWithIndex and filter\"\n",
    "This is an extended version of filter. It takes two function arguments. The first argument\n",
    "must conform to Int ⇒ T and is executed once per partition. It will transform the\n",
    "partition index to type T . The second function looks like (U, T ) ⇒ Boolean. T is\n",
    "the transformed partition index and U are the data items from the RDD. Finally the\n",
    "function has to return either true or false (i.e. Apply the filter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,4,6,8,10\n",
      "1,2,3,4,5,6,7,8,9,10\n",
      "1,2,3\n"
     ]
    }
   ],
   "source": [
    "val a = sc.parallelize (1 to 10 , 3)\n",
    "println(a.filter( _ % 2 == 0).collect.mkString(\",\"))\n",
    "println(a.collect.mkString(\",\"))\n",
    "\n",
    "val b = sc.parallelize(1 to 8)\n",
    "println(b.filter(_ < 4).collect.mkString(\",\"))\n",
    "\n",
    "\n",
    "//Error\n",
    "// val a1 = sc . parallelize ( List (\" cat \" , \" horse \" , 4.0 , 3.5 , 2 , \" dog \") )\n",
    "//a1 . filter ( _ < 4) . collect\n",
    "/*This fails because some components of a are not implicitly comparable against integers.\n",
    "  Collect uses the isDefinedAt property of a function-object to determine whether the test-\n",
    "  function is compatible with each data item. Only data items that pass this test (=filter)\n",
    "are then mapped using the function-object. */\n",
    "\n",
    "val a2 = sc.parallelize ( List (\" cat \" , \" horse \" , 4.0 , 3.5 , 2 , \" dog \") )\n",
    "a2 . collect ({ case a : Int => \" is integer \"\n",
    "case b : String => \" is string \" }) . collect\n",
    "\n",
    "val myfunc : PartialFunction [ Any , Any ] = {\n",
    "  case a : Int  => \" is integer \"\n",
    "  case b : String => \" is string \" }\n",
    "\n",
    "myfunc.isDefinedAt(\"\")\n",
    "myfunc.isDefinedAt(1)\n",
    "myfunc.isDefinedAt(1.5)\n",
    "\n",
    "//    Be careful! The above code works because it only checks the type itself! If you use\n",
    "//    operations on this type, you have to explicitly declare what type you want instead of any.\n",
    "//    Otherwise the compiler does (apparently) not know what bytecode it should produce:\n",
    "\n",
    "//val myfunc2 : PartialFunction [ Any , Any ] = { case x if ( x < 4) => \" x \"} //error\n",
    "val myfunc3 : PartialFunction[Int,Any] = { case x if (x < 4) => \"x\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo closure: rslt1: 3 rslt2 : 3\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"A\",\"B\",\"C\",\"D\"))\n",
    "val str1 = \"A\"\n",
    "\n",
    "val rslt1 = rdd.filter(x => {x != \"A\"}).count\n",
    "val rslt2 = rdd.filter(x => { str1 != null && x != str1}).count\n",
    "\n",
    "println(\"Demo closure: rslt1: \" + rslt1 + \" rslt2 : \" + rslt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct\n",
    "Returns a new RDD that contains each unique value only once.\n",
    "\n",
    "SELECT DISTINCT * FROM table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$iw\n",
      "Dog                                                                             \n",
      "Cat\n",
      "Gnu\n",
      "Rat\n",
      "2\n",
      "3\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "println(this.getClass.getSimpleName)\n",
    "val c = sc.parallelize(List(\"Gnu\",\"Cat\",\"Rat\",\"Dog\",\"Gnu\",\"Rat\") ,2)\n",
    "c.distinct.collect.foreach(println)\n",
    "\n",
    "val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10,1,2,3,4,5))\n",
    "println(a.distinct(2).partitions.length) //2\n",
    "println(a.distinct(3).partitions.length) //3\n",
    "println(\"-------------------------------------------------------\")\n",
    "\n",
    "a.distinct(3).foreachPartition(p => {\n",
    "    p.foreach(println)\n",
    "    println(\"-------------------------------------------------------\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "false\n"
     ]
    }
   ],
   "source": [
    "import breeze.util.BloomFilter\n",
    "val nums = List(1 to 20: _*).map(_.toString)\n",
    "val rdd = sc.parallelize(nums,5)\n",
    "\n",
    "val bf = rdd.mapPartitions{ iter =>\n",
    "val bf = BloomFilter.optimallySized[String](10000, 0.0001)\n",
    "    iter.foreach(i => bf += i)\n",
    "    Iterator(bf)\n",
    "}.reduce(_ | _)\n",
    "\n",
    "println(bf.contains(\"5\"))\n",
    "println(bf.contains(\"31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToreeSpark - Scala",
   "language": "scala",
   "name": "toreespark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
