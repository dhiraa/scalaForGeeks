{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note Outputs are not in order due to Toree bug!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a distributed computing engine and its main abstraction is a resilient distributed dataset (RDD), which can be\n",
    "viewed as a distributed collection. Basically, RDD's elements are partitioned across the nodes of the cluster, but Spark\n",
    "abstracts this away from the user, letting the user interact with the RDD (collection) as if it were a local one.\n",
    "\n",
    "Not to get into too many details, but when you run different transformations on a RDD (map, flatMap, filter and others),\n",
    "your transformation code (closure) is:\n",
    "\n",
    "1. Serialized on the driver node,\n",
    "2. Shipped to the appropriate nodes in the cluster,\n",
    "3. Deserialized, and\n",
    "4. Finally executed on the nodes\n",
    "\n",
    "You can of course run this locally, but all those phases (apart from shipping over network) still\n",
    "occur. [This lets you catch any bugs even before deploying to production]\n",
    "\n",
    "Links: http://stackoverflow.com/questions/22592811/task-not-serializable-java-io-notserializableexception-when-calling-function-ou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleRDDManipulation\n"
     ]
    }
   ],
   "source": [
    "val listRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "def increment(x: Int) = x + 1\n",
    "println(\"simpleRDDManipulation\")\n",
    "val incrementedListRDD = listRDD.map(increment(_))\n",
    "incrementedListRDD.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "//You know I am going to take your application down ;)\n",
    "class ClassRDDManupulation_Ver1 {\n",
    "  def listIncrement(sc: org.apache.spark.SparkContext) = {\n",
    "\n",
    "    val listRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "    def increment(x: Int) = x + 1\n",
    "    println(\"ClassRDDManupulation_Ver1\")\n",
    "    val incrementedListRDD = listRDD.map(increment(_))\n",
    "    incrementedListRDD.foreach(println)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassRDDManupulation_Ver1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)\n",
       "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2290)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:369)\n",
       "  at ClassRDDManupulation_Ver1.listIncrement(<console>:22)\n",
       "  ... 42 elided\n",
       "Caused by: java.io.NotSerializableException: ClassRDDManupulation_Ver1\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: ClassRDDManupulation_Ver1, value: ClassRDDManupulation_Ver1@2559490e)\n",
       "\t- field (class: ClassRDDManupulation_Ver1$$anonfun$1, name: $outer, type: class ClassRDDManupulation_Ver1)\n",
       "\t- object (class ClassRDDManupulation_Ver1$$anonfun$1, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "5\n",
      "8\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "//So why did this made the crash? Neither the class or the function is serializable\n",
    "val classRDDManupulation_Ver1 = new ClassRDDManupulation_Ver1\n",
    "classRDDManupulation_Ver1.listIncrement(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassRDDManupulation_Ver2 extends java.io.Serializable { //Someone packed me! huh I am gonna travel\n",
    "    def listIncrement(sc : org.apache.spark.SparkContext) = {\n",
    "\n",
    "      val listRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "      def increment(x: Int) = x + 10\n",
    "      println(\"ClassRDDManupulation_Ver2\")\n",
    "      val incrementedListRDD = listRDD.map(increment(_))\n",
    "      incrementedListRDD.foreach(println)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassRDDManupulation_Ver2\n",
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "val classRDDManupulation_Ver2 = new ClassRDDManupulation_Ver2\n",
    "classRDDManupulation_Ver2.listIncrement(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Only the increment anonymous function is serialized by default by Spark\n",
    "class ClassRDDManupulation_Ver3 {\n",
    "  def listIncrement(sc : org.apache.spark.SparkContext) = {\n",
    "\n",
    "    val listRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "    val increment = (x: Int) => x + 100 //Hey there... now I am a first class citizen!\n",
    "    println(\"ClassRDDManupulation_Ver3\")\n",
    "    val incrementedListRDD = listRDD.map(increment(_))\n",
    "    incrementedListRDD.foreach(println)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassRDDManupulation_Ver3\n"
     ]
    }
   ],
   "source": [
    "val classRDDManupulation_Ver3 = new ClassRDDManupulation_Ver3\n",
    "classRDDManupulation_Ver3.listIncrement(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//By default whole obejct is serialized\n",
    "object ClassRDDManupulation_Ver4 {\n",
    "  def listIncrement(sc : org.apache.spark.SparkContext) = {\n",
    "\n",
    "    val listRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)\n",
    "    def increment(x: Int) = x + 1000\n",
    "    println(\"ClassRDDManupulation_Ver4\")\n",
    "    val incrementedListRDD = listRDD.map(increment(_))\n",
    "    incrementedListRDD.foreach(println)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassRDDManupulation_Ver4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)\n",
       "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2290)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:369)\n",
       "  at ClassRDDManupulation_Ver4$.listIncrement(<console>:35)\n",
       "  ... 42 elided\n",
       "Caused by: java.io.NotSerializableException: ClassRDDManupulation_Ver4$\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: ClassRDDManupulation_Ver4$, value: ClassRDDManupulation_Ver4$@5e1228ad)\n",
       "\t- field (class: ClassRDDManupulation_Ver4$$anonfun$1, name: $outer, type: class ClassRDDManupulation_Ver4$)\n",
       "\t- object (class ClassRDDManupulation_Ver4$$anonfun$1, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClassRDDManupulation_Ver4.listIncrement(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToreeSpark - Scala",
   "language": "scala",
   "name": "toreespark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
